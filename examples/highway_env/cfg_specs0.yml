specs: hw-env_specs.stl

real_time_step: 1.0
BigM: 10

action_names:
  action: "action"
obs_names:
  ego_presence: "obs[0][0]"
  ego_x: "obs[0][1]"
  ego_y: "obs[0][2]"
  ego_vx: "80*obs[0][3]"
  ego_vy: "80*obs[0][4]"
  car1_presence: "obs[1][0]"
  car1_x: "obs[1][1]"
  car1_y: "obs[1][2]"
  car1_vx: "80*obs[1][3]"
  car1_vy: "80*obs[1][4]"
  car2_presence: "obs[2][0]"
  car2_x: "obs[2][1]"
  car2_y: "obs[2][2]"
  car2_vx: "80*obs[2][3]"
  car2_vy: "80*obs[2][4]"
  car3_presence: "obs[3][0]"
  car3_x: "obs[3][1]"
  car3_y: "obs[3][2]"
  car3_vx: "80*obs[3][3]"
  car3_vy: "80*obs[3][4]"

# linear combination .. new_reward = reward + w1 rho1 + w2 rho2 etc
reward_formulas:
  ego_fast:
    past_horizon: 0
    weight: .1
  danger:
    past_horizon: 0
    weight: -20
    lower_bound: 0.0 # we don't want to reward safety too much

# Same for everybody
eval_formulas: cfg_eval.yml

