{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import *\n",
    "from stable_baselines3 import PPO,A2C,SAC,TD3,DQN,DDPG\n",
    "from stable_baselines3.common.save_util import load_from_zip_file\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "from pprint import pprint\n",
    "import enum\n",
    "\n",
    "import rlrom.wrappers.stl_wrapper\n",
    "import stlrom\n",
    "from rlrom.envs import *\n",
    "import rlrom.utils\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EnvMode(enum.Enum):\n",
    "    VANILLA=0\n",
    "    TERM_SLOW=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_mode= EnvMode.VANILLA\n",
    "collision_reward = -1\n",
    "model_name = 'ppo_hw_van_high_col.zip'\n",
    "\n",
    "\n",
    "def make_env(train=True, env_mode=env_mode, verbose=0):\n",
    "\n",
    "    if train:\n",
    "        env = gym.make(\"highway-fast-v0\")\n",
    "    else:\n",
    "        env = gym.make(\"highway-v0\", render_mode='human')\n",
    "\n",
    "    env.unwrapped.configure({\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.4,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 1.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": True,\n",
    "                \"offroad_terminal\": False,        \n",
    "    })\n",
    "\n",
    "    if env_mode==EnvMode.TERM_SLOW:\n",
    "        cfg = cfg_envs['highway-env']\n",
    "        driver= stlrom.STLDriver()\n",
    "        driver.parse_string(cfg['specs'])        \n",
    "        env = rlrom.wrappers.stl_wrapper.STLWrapper(env,driver,signals_map=cfg, terminal_formulas={'ego_slow_too_long'})\n",
    "\n",
    "    if verbose>=1:\n",
    "        pprint(cfg)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 12\n",
    "batch_size = 64\n",
    "neurons = 128\n",
    "policy_kwargs = dict(\n",
    "    #activation_fn=th.nn.ReLU,\n",
    "    net_arch=dict(pi=[neurons, neurons], qf=[neurons, neurons])\n",
    ")\n",
    "\n",
    "vec_env = make_vec_env(make_env, n_envs=n_cpu, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\n",
    "     \"MlpPolicy\",\n",
    "     vec_env,\n",
    "     device='cpu',\n",
    "     policy_kwargs=policy_kwargs,\n",
    "     n_steps=batch_size * 12 // n_cpu,\n",
    "     batch_size=batch_size,\n",
    "     n_epochs=10,\n",
    "     learning_rate=5e-4,\n",
    "     gamma=0.9,\n",
    "     verbose=1,\n",
    "     tensorboard_log=\"./highway_ppo/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=200_000,\n",
    "    progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ppo_model_slow_term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.rollout_buffer.observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_env(train=False,env_mode=env_mode, verbose=0)\n",
    "env.unwrapped.configure({\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.1,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 2.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0.,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": False,\n",
    "                \"offroad_terminal\": False,\n",
    "                \"manual_control\": False        \n",
    "    })\n",
    "\n",
    "#obs, info = env.reset(seed=1)\n",
    "obs, info = env.reset()\n",
    "#env.stl_driver.set_param('v_slow', 0.3)\n",
    "#env.stl_driver.set_param('v_fast', 0.35)\n",
    "#wobs = env.wrapped_obs\n",
    "for _ in range(100):    \n",
    "    #action, _states = model.predict(wobs)\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)    \n",
    "    #wobs= env.wrapped_obs\n",
    "\n",
    "    if terminated:\n",
    "        print('Crash')\n",
    "        break    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay = \"\"\"\n",
    " action\n",
    " ego_x_fast\n",
    " reward\n",
    " \"\"\"\n",
    "lay = utils.get_layout_from_string(lay)\n",
    "\n",
    "width = 12\n",
    "height = 4\n",
    "fig, axs = plt.subplots(len(lay),1, figsize=(width, height))\n",
    "\n",
    "idx_ax =0\n",
    "for sig_list in lay:\n",
    "    for sig in sig_list:\n",
    "        if len(lay)>1:\n",
    "            env.plot_signal(sig, axs[idx_ax])\n",
    "        else:\n",
    "            env.plot_signal(sig, axs)\n",
    "    idx_ax +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
