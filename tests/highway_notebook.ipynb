{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import *\n",
    "from stable_baselines3 import PPO,A2C,SAC,TD3,DQN,DDPG\n",
    "from stable_baselines3.common.save_util import load_from_zip_file\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "from pprint import pprint\n",
    "import enum\n",
    "\n",
    "import rlrom.wrappers.stl_wrapper\n",
    "import stlrom\n",
    "from rlrom.envs import *\n",
    "import rlrom.utils\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMode(enum.Enum):\n",
    "    VANILLA=0\n",
    "    TERM_SLOW=1\n",
    "    \n",
    "def make_env(train=True, env_mode=EnvMode.TERM_SLOW, verbose=0):\n",
    "    if train:\n",
    "        env = gym.make(\"highway-fast-v0\")\n",
    "    else:\n",
    "        env = gym.make(\"highway-v0\", render_mode='human')\n",
    "\n",
    "    env.unwrapped.configure({\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.4,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 1.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": True,\n",
    "                \"offroad_terminal\": False,        \n",
    "    })\n",
    "\n",
    "    if env_mode==EnvMode.TERM_SLOW:\n",
    "        cfg = cfg_envs['highway-env']\n",
    "        driver= stlrom.STLDriver()\n",
    "        driver.parse_string(cfg['specs'])        \n",
    "        env = rlrom.wrappers.stl_wrapper.STLWrapper(env,driver,signals_map=cfg, terminal_formulas={'ego_slow_too_long'})\n",
    "\n",
    "    if verbose>=1:\n",
    "        pprint(cfg)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training vanilla highway-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 12\n",
    "batch_size = 64\n",
    "neurons = 128\n",
    "policy_kwargs = dict(\n",
    "    #activation_fn=th.nn.ReLU,\n",
    "    net_arch=dict(pi=[neurons, neurons], qf=[neurons, neurons])\n",
    ")\n",
    "\n",
    "vec_env = make_vec_env(make_env, n_envs=n_cpu, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\n",
    "     \"MlpPolicy\",\n",
    "     vec_env,\n",
    "     device='cpu',\n",
    "     policy_kwargs=policy_kwargs,\n",
    "     n_steps=batch_size * 12 // n_cpu,\n",
    "     batch_size=batch_size,\n",
    "     n_epochs=10,\n",
    "     learning_rate=7e-4,\n",
    "     gamma=0.9,\n",
    "     verbose=1,\n",
    "     tensorboard_log=\"./highway_ppo/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=100_000,\n",
    "    progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('ppo_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('ppo_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_mode = EnvMode.TERM_SLOW\n",
    "env = make_env(train=False,env_mode=EnvMode.TERM_SLOW, verbose=0)\n",
    "env.unwrapped.configure({\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.1,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 0.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0.,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": False,\n",
    "                \"offroad_terminal\": False,\n",
    "                \"manual_control\": True        \n",
    "    })\n",
    "\n",
    "obs, info = env.reset()\n",
    "env.stl_driver.set_param('v_slow', 0.3)\n",
    "env.stl_driver.set_param('v_fast', 0.35)\n",
    "#wobs = env.wrapped_obs\n",
    "for _ in range(100):    \n",
    "    #action, _states = model.predict(wobs, deterministic=True)\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)    \n",
    "    wobs= env.wrapped_obs\n",
    "\n",
    "    if terminated:\n",
    "        print('Crash')\n",
    "        break    \n",
    "env.close()\n",
    "\n",
    "lay = \"\"\"\n",
    " ego_x\n",
    " ego_slow\n",
    " ego_slow_too_long\n",
    "\"\"\"\n",
    "lay = utils.get_layout_from_string(lay)\n",
    "\n",
    "width = 12\n",
    "height = 4\n",
    "fig, axs = plt.subplots(len(lay),1, figsize=(width, height))\n",
    "\n",
    "idx_ax =0\n",
    "for sig_list in lay:\n",
    "    for sig in sig_list:\n",
    "        if len(lay)>1:\n",
    "            env.plot_signal(sig, axs[idx_ax])\n",
    "        else:\n",
    "            env.plot_signal(sig, axs)\n",
    "    idx_ax +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "env.plot_signal(\"reward\", ax)\n",
    "# Show the plot with interactive features\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fig(env, signals_layout):\n",
    "    lay = utils.get_layout_from_string(signals_layout)\n",
    "    status = \"Plot ok. Hit reset on top right if not visible.\"            \n",
    "    #f= figure(height=200)\n",
    "    figs = []\n",
    "    colors = itertools.cycle(palette)    \n",
    "    for signal_list in enumerate(lay):\n",
    "        f=None\n",
    "        for signal in signal_list[1]:                \n",
    "            #try: \n",
    "                color=colors.__next__()                    \n",
    "                #tr_idx = self.trace_idx\n",
    "                print(signal.strip())\n",
    "                if signal.strip().startswith(\"set_trace_idx(\") or signal.strip().startswith(\"_tr(\"):            \n",
    "                    tr_idx = int(signal.split('(')[1][:-1])                         \n",
    "                    env.set_current_trace(tr_idx)                        \n",
    "                else: \n",
    "                    if f is None:\n",
    "                        if figs == []:\n",
    "                            f = figure(height=200)\n",
    "                        else:\n",
    "                            f = figure(height=200, x_range=figs[0][0].x_range)\n",
    "                        figs.append([f])\n",
    "                    env.plot_signal(f, signal, color=color)\n",
    "            #except:\n",
    "            #     status = \"Warning: error getting values for \" + signal\n",
    "    fig = gridplot(figs, sizing_mode='stretch_width')        \n",
    "    \n",
    "    return fig, status\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_plot_signal(self, signal, fig=None,label=None,  color=None, online=False, horizon=0):\n",
    "    # signal should be part of the \"signal\" declaration or a valid formula id \n",
    "     \n",
    "        if self.stl_driver.data == []:\n",
    "            raise ValueError(\"No data to plot.\")\n",
    "                 \n",
    "        time = self.get_time()\n",
    "\n",
    "        if signal in self.signals_map:\n",
    "            signal_index = list(self.signals_map.keys()).index(signal)+1        \n",
    "            sig_values = [s[signal_index] for s in self.stl_driver.data]\n",
    "            if label is None:\n",
    "                label=signal\n",
    "        elif signal in self.formulas:\n",
    "            sig_values = self.get_rob(signal, online=online,horizon=horizon)\n",
    "            signal_index = self.formulas.index(signal)+len(self.signals_map)        \n",
    "            if label is None:\n",
    "                label=signal\n",
    "        elif isinstance(signal, np.ndarray) and signal.shape == (len(self.get_time()),):\n",
    "            sig_values = signal\n",
    "        elif isinstance(signal, stlrom.Signal):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                sig_values = self.get_rob(signal, online=online,horizon=horizon)\n",
    "            except Exception as e:\n",
    "               raise ValueError(f\"Name '{signal}' not in signals_map nor in parsed formulas\")\n",
    "\n",
    "        if fig is None:\n",
    "            fig = figure(height=200)\n",
    "\n",
    "        fig.xaxis.axis_label = \"Time\"\n",
    "        print(sig_values)\n",
    "        fig.step(time, sig_values)            \n",
    "        \n",
    "        \n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = figure(height=200)\n",
    "db_plot_signal(env, \"ego_x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, status = get_fig(env, lay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
