{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2025-05-16 16:06:25.651139: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 16:06:25.796307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-16 16:06:25.850471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-16 16:06:25.867507: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 16:06:25.973542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 16:06:26.677724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from context import *\n",
    "from stable_baselines3 import PPO,A2C,SAC,TD3,DQN,DDPG\n",
    "from stable_baselines3.common.save_util import load_from_zip_file\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "from pprint import pprint\n",
    "import enum\n",
    "\n",
    "import rlrom.wrappers.stl_wrapper\n",
    "import stlrom\n",
    "from rlrom.envs import *\n",
    "import rlrom.utils\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "\n",
    "class EnvMode(enum.Enum):\n",
    "    VANILLA=0\n",
    "    TERM_SLOW=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_mode= EnvMode.VANILLA\n",
    "collision_reward = -1\n",
    "model_name = 'ppo_hw_van_high_col.zip'\n",
    "\n",
    "cfg_hw_env = {\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.4,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 1.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": True,\n",
    "                \"offroad_terminal\": False,        \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stl_mode': 'None', 'model_path': './models', 'model_name': 'ppo_hw_van_high_col.zip', 'cfg_hw_env': {'observation': {'type': 'Kinematics'}, 'action': {'type': 'DiscreteMetaAction'}, 'lanes_count': 4, 'vehicles_count': 50, 'controlled_vehicles': 1, 'initial_lane_id': None, 'duration': 100, 'ego_spacing': 2, 'vehicles_density': 1, 'collision_reward': -0.4, 'right_lane_reward': 0, 'high_speed_reward': 1.0, 'lane_change_reward': 0, 'reward_speed_range': [20, 30], 'normalize_reward': True, 'offroad_terminal': False}}\n"
     ]
    }
   ],
   "source": [
    "cfg = dict()\n",
    "cfg['stl_mode'] = 'None'\n",
    "cfg['model_path'] = './models'\n",
    "cfg['model_name'] = 'ppo_hw_van_high_col.zip'\n",
    "cfg['cfg_hw_env'] = cfg_hw_env\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a YAML file\n",
    "with open(\"cfg_hw.yaml\", \"w\") as file:\n",
    "    yaml.dump(cfg_hw, file,sort_keys=False, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation': {'type': 'Kinematics'}, 'action': {'type': 'DiscreteMetaAction'}, 'lanes_count': 4, 'vehicles_count': 50, 'controlled_vehicles': 1, 'initial_lane_id': None, 'duration': 100, 'ego_spacing': 2, 'vehicles_density': 1, 'collision_reward': -2.0, 'right_lane_reward': 0.0, 'high_speed_reward': 1.0, 'lane_change_reward': 0, 'reward_speed_range': [20, 30], 'normalize_reward': True, 'offroad_terminal': False}\n"
     ]
    }
   ],
   "source": [
    "with open(\"cfg_hw.yaml\", \"r\") as file:\n",
    "    cfg_hw2 = yaml.safe_load(file)\n",
    "print(cfg_hw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(train=True, env_mode=env_mode, verbose=0):\n",
    "\n",
    "    if train:\n",
    "        env = gym.make(\"highway-fast-v0\")\n",
    "    else:\n",
    "        env = gym.make(\"highway-v0\", render_mode='human')\n",
    "\n",
    "    env.unwrapped.configure(cfg_hw)\n",
    "\n",
    "    if env_mode==EnvMode.TERM_SLOW:\n",
    "        cfg = cfg_envs['highway-env']\n",
    "        driver= stlrom.STLDriver()\n",
    "        driver.parse_string(cfg['specs'])        \n",
    "        env = rlrom.wrappers.stl_wrapper.STLWrapper(env,driver,signals_map=cfg, terminal_formulas={'ego_slow_too_long'})\n",
    "\n",
    "    if verbose>=1:\n",
    "        pprint(cfg)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 12\n",
    "batch_size = 64\n",
    "neurons = 128\n",
    "policy_kwargs = dict(\n",
    "    #activation_fn=th.nn.ReLU,\n",
    "    net_arch=dict(pi=[neurons, neurons], qf=[neurons, neurons])\n",
    ")\n",
    "\n",
    "vec_env = make_vec_env(make_env, n_envs=n_cpu, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\n",
    "     \"MlpPolicy\",\n",
    "     vec_env,\n",
    "     device='cpu',\n",
    "     policy_kwargs=policy_kwargs,\n",
    "     n_steps=batch_size * 12 // n_cpu,\n",
    "     batch_size=batch_size,\n",
    "     n_epochs=10,\n",
    "     learning_rate=5e-4,\n",
    "     gamma=0.9,\n",
    "     verbose=1,\n",
    "     tensorboard_log=\"./highway_ppo/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=200_000,\n",
    "    progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ppo_model_slow_term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.rollout_buffer.observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_env(train=False,env_mode=env_mode, verbose=0)\n",
    "env.unwrapped.configure({\n",
    "            \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 100,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -.1,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 2.,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0.,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": False,\n",
    "                \"offroad_terminal\": False,\n",
    "                \"manual_control\": False        \n",
    "    })\n",
    "\n",
    "#obs, info = env.reset(seed=1)\n",
    "obs, info = env.reset()\n",
    "#env.stl_driver.set_param('v_slow', 0.3)\n",
    "#env.stl_driver.set_param('v_fast', 0.35)\n",
    "#wobs = env.wrapped_obs\n",
    "for _ in range(100):    \n",
    "    #action, _states = model.predict(wobs)\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)    \n",
    "    #wobs= env.wrapped_obs\n",
    "\n",
    "    if terminated:\n",
    "        print('Crash')\n",
    "        break    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay = \"\"\"\n",
    " action\n",
    " ego_x_fast\n",
    " reward\n",
    " \"\"\"\n",
    "lay = utils.get_layout_from_string(lay)\n",
    "\n",
    "width = 12\n",
    "height = 4\n",
    "fig, axs = plt.subplots(len(lay),1, figsize=(width, height))\n",
    "\n",
    "idx_ax =0\n",
    "for sig_list in lay:\n",
    "    for sig in sig_list:\n",
    "        if len(lay)>1:\n",
    "            env.plot_signal(sig, axs[idx_ax])\n",
    "        else:\n",
    "            env.plot_signal(sig, axs)\n",
    "    idx_ax +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
