{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 09:29:38.445186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-27 09:29:38.460497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-27 09:29:38.465012: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-27 09:29:38.476077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-27 09:29:39.206675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/alex/python_venvs/rlrom_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from rlrom.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendul_cfg = cfg_envs['Pendulum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# Create the Pendulum environment\n",
    "env = gym.make(pendul_cfg['env_name'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# --- Parameters ---\n",
    "ENV_ID = \"Pendulum-v1\"\n",
    "# Hyperparameters inspired by RL Baselines Zoo3 for SAC on Pendulum-v1\n",
    "# (Note: Zoo often uses more advanced setups like VecNormalize, but this provides the core SAC params)\n",
    "tuned_params = {\n",
    "    'learning_rate': 7.3e-4, # Often represented as linear_schedule(7.3e-4) in the zoo, but constant works too\n",
    "    'buffer_size': 50000,\n",
    "    'learning_starts': 100,\n",
    "    'batch_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 1,\n",
    "    'gradient_steps': 1,\n",
    "    'ent_coef': 'auto', # Automatically tune entropy coefficient\n",
    "    'target_update_interval': 1,\n",
    "    'target_entropy': 'auto',\n",
    "    'use_sde': False, # State Dependent Exploration - can sometimes help, but often not needed for Pendulum\n",
    "    'sde_sample_freq': -1,\n",
    "    'policy_kwargs': dict(net_arch=[64, 64]) # Neural network architecture\n",
    "}\n",
    "TOTAL_TIMESTEPS = 50_000 # Adjust as needed, 50k-100k is often enough for Pendulum\n",
    "N_EVAL_EPISODES = 10\n",
    "MODEL_SAVE_PATH = \"./sac_pendulum_tuned\"\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Create a single environment instance, wrapped with Monitor for logging\n",
    "env = gym.make(ENV_ID)\n",
    "env = Monitor(env)\n",
    "\n",
    "# For evaluation later\n",
    "eval_env = gym.make(ENV_ID)\n",
    "eval_env = Monitor(eval_env) # Wrap eval env too if you want stats from evaluation\n",
    "\n",
    "\n",
    "# --- Model Definition ---\n",
    "# Use the tuned hyperparameters\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1, # Print training progress\n",
    "    **tuned_params\n",
    ")\n",
    "\n",
    "print(f\"--- Training SAC on {ENV_ID} for {TOTAL_TIMESTEPS} timesteps ---\")\n",
    "print(f\"Hyperparameters: {tuned_params}\")\n",
    "\n",
    "# --- Training ---\n",
    "# The learn() method handles the training loop\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=10) # Log every 10 training updates\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "# --- Saving the Model ---\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}.zip\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"--- Evaluating Trained Agent ---\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=N_EVAL_EPISODES)\n",
    "print(f\"Evaluation over {N_EVAL_EPISODES} episodes:\")\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# --- Optional: Load and Run Trained Agent ---\n",
    "# del model # Remove trained model from memory\n",
    "# loaded_model = SAC.load(MODEL_SAVE_PATH, env=eval_env)\n",
    "# print(\"--- Running Loaded Agent ---\")\n",
    "# obs, _ = eval_env.reset()\n",
    "# total_reward = 0\n",
    "# terminated = False\n",
    "# truncated = False\n",
    "# while not terminated and not truncated:\n",
    "#     action, _states = loaded_model.predict(obs, deterministic=True) # Use deterministic actions for evaluation\n",
    "#     obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "#     total_reward += reward\n",
    "#     # Optional: render the environment\n",
    "#     # eval_env.render()\n",
    "# print(f\"Reward in one episode run: {total_reward}\")\n",
    "\n",
    "eval_env.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import stlrom\n",
    "import numpy as np\n",
    "from rlrom.wrappers import STLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/python_venvs/rlrom_env/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env= gym.make(cfg_envs['Pendulum']['env_name'], render_mode= 'human')\n",
    "loaded_model = PPO.load(\"ppo_pendulum_optimized\", env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = stlrom.STLDriver()\n",
    "driver.parse_string(pendul_cfg['specs'])\n",
    "env = STLWrapper(env, driver, signals_map=pendul_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torque': 'action[0]', 'cos_theta': 'obs[0]', 'sin_theta': 'obs[1]', 'theta_dot': 'obs[2]', 'reward': 'reward'}\n"
     ]
    }
   ],
   "source": [
    "print(env.signals_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,_= env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during test: -135.08449865717606\n"
     ]
    }
   ],
   "source": [
    "obs,_= env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward =0\n",
    "while not terminated and not truncated:\n",
    "    action, _states = loaded_model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()  # Optional: render the environment\n",
    "\n",
    "print(f\"Total reward during test: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlrom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
