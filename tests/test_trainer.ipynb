{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import stlrom\n",
    "import numpy as np\n",
    "from rlrom.wrappers import STLWrapper\n",
    "from rlrom.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg_envs['Pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'LunarLander', 'highway-env', 'MountainCarContinuous', 'CartPole', 'Pendulum', 'MountainCar', 'LunarLanderContinuous', 'BipedalWalker', 'CarRacing', 'BipedalWalkerHardcore', 'Acrobot']\n"
     ]
    }
   ],
   "source": [
    "print(['']+supported_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(cfg['env_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# --- Parameters ---\n",
    "ENV_ID = \"Pendulum-v1\"\n",
    "# Hyperparameters inspired by RL Baselines Zoo3 for SAC on Pendulum-v1\n",
    "# (Note: Zoo often uses more advanced setups like VecNormalize, but this provides the core SAC params)\n",
    "tuned_params = {\n",
    "    'learning_rate': 7.3e-4, # Often represented as linear_schedule(7.3e-4) in the zoo, but constant works too\n",
    "    'buffer_size': 50000,\n",
    "    'learning_starts': 100,\n",
    "    'batch_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 1,\n",
    "    'gradient_steps': 1,\n",
    "    'ent_coef': 'auto', # Automatically tune entropy coefficient\n",
    "    'target_update_interval': 1,\n",
    "    'target_entropy': 'auto',\n",
    "    'use_sde': False, # State Dependent Exploration - can sometimes help, but often not needed for Pendulum\n",
    "    'sde_sample_freq': -1,\n",
    "    'policy_kwargs': dict(net_arch=[64, 64]) # Neural network architecture\n",
    "}\n",
    "TOTAL_TIMESTEPS = 50_000 # Adjust as needed, 50k-100k is often enough for Pendulum\n",
    "N_EVAL_EPISODES = 10\n",
    "MODEL_SAVE_PATH = \"./sac_pendulum_tuned\"\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Create a single environment instance, wrapped with Monitor for logging\n",
    "env = gym.make(ENV_ID)\n",
    "env = Monitor(env)\n",
    "\n",
    "# For evaluation later\n",
    "eval_env = gym.make(ENV_ID)\n",
    "eval_env = Monitor(eval_env) # Wrap eval env too if you want stats from evaluation\n",
    "\n",
    "\n",
    "# --- Model Definition ---\n",
    "# Use the tuned hyperparameters\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1, # Print training progress\n",
    "    **tuned_params\n",
    ")\n",
    "\n",
    "print(f\"--- Training SAC on {ENV_ID} for {TOTAL_TIMESTEPS} timesteps ---\")\n",
    "print(f\"Hyperparameters: {tuned_params}\")\n",
    "\n",
    "# --- Training ---\n",
    "# The learn() method handles the training loop\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=10) # Log every 10 training updates\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "# --- Saving the Model ---\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}.zip\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"--- Evaluating Trained Agent ---\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=N_EVAL_EPISODES)\n",
    "print(f\"Evaluation over {N_EVAL_EPISODES} episodes:\")\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# --- Optional: Load and Run Trained Agent ---\n",
    "# del model # Remove trained model from memory\n",
    "# loaded_model = SAC.load(MODEL_SAVE_PATH, env=eval_env)\n",
    "# print(\"--- Running Loaded Agent ---\")\n",
    "# obs, _ = eval_env.reset()\n",
    "# total_reward = 0\n",
    "# terminated = False\n",
    "# truncated = False\n",
    "# while not terminated and not truncated:\n",
    "#     action, _states = loaded_model.predict(obs, deterministic=True) # Use deterministic actions for evaluation\n",
    "#     obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "#     total_reward += reward\n",
    "#     # Optional: render the environment\n",
    "#     # eval_env.render()\n",
    "# print(f\"Reward in one episode run: {total_reward}\")\n",
    "\n",
    "eval_env.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with STLWrapper only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env= gym.make(cfg['env_name'], render_mode= 'human')\n",
    "#loaded_model = PPO.load(\"ppo_pendulum_optimized\", env=env)\n",
    "loaded_model = SAC.load(\"sac_pendulum_tuned\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = stlrom.STLDriver()\n",
    "driver.parse_string(cfg['specs'])\n",
    "env = STLWrapper(env, driver, signals_map=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during test: -239.28462995359527\n"
     ]
    }
   ],
   "source": [
    "obs,_= env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward =0\n",
    "while not terminated and not truncated:\n",
    "    action, _states = loaded_model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()  # Optional: render the environment\n",
    "\n",
    "print(f\"Total reward during test: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with RLModelTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlrom.testers import RLModelTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_name = cfg['env_name']\n",
    "model_name = 'sac_pendulum_tuned.zip'\n",
    "tester = RLModelTester()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlrom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
